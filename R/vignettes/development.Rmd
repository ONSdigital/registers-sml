---
title: "Development Advice"
author: "Nathan Eastwood"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Development Advice}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  collapse = TRUE,
  comment = "#"
)
library(sparkts)
library(testthat)
```

# Set up

NOTE: When cloning the repository to an on-network drive, you MUST clone it to the D drive; failure to do so will not allow you to build the package.



If you have cloned `sparkts` and wish to `build` it, you will need:

* R (>= 3.4.3)
* RStudio
* RTools34

You will then need to run the following commands (note we need the development version of `sparklyr` due to bugs in the version available from CRAN):

```{r build_infrastructure, eval = FALSE}
install.packages(c("dplyr", "devtools", "testthat", "covr", "knitr", "rmarkdown"))
source("https://raw.githubusercontent.com/r-lib/remotes/master/install-github.R")$value("r-lib/remotes")
remotes::install_github("rstudio/sparklyr")
```


You also need to install Spark. On Windows machines you need to ensure your LOCALAPPDATA environment variable is set to the correct folder. First check if this is already set

```{r localloc}
Sys.getenv()["LOCALAPPDATA"]
```

If this is `""`, you can set it within your `Renviron.site` file. This is located in `C:\Applications\RStudio\R-3.4.3\etc` (or similar). Within this file, on a separate line, type the following:

```
LOCALAPPDATA="C:/Users/username/AppData/Local"
```

Where `username` is your Windows login username. Now Spark should install in the correct directory

```{r spark_install, eval = FALSE}
sparklyr::spark_install(version = "2.2.0")
```

You can check where Spark has installed with

```{r spark_location, eval = FALSE}
sparklyr::spark_install_dir()
```

Sometimes on ONS systems you will need to set or edit your `HADOOP_HOME` environment variable. You can set this using

```{r hadoop_home, eval = FALSE}
Sys.setenv(HADOOP_HOME = "C:\Applications\RStudio\bin\winutils\x64")
```


Note that you may come across [firewall restrictions](https://github.com/rstudio/sparklyr/issues/1039#issuecomment-333586743) when trying to connect to Spark. To get around this we can use the localhost instead (assuming this isn't also blocked).

```{r spark_connect, eval = FALSE}
sc <- sparklyr::spark_connect(
  master = "local",
  version = "2.2.0",
  config = list(sparklyr.gateway.address = "127.0.0.1")
)
```


Once you have all of your dependencies correctly setup, you can then build and install the package using

```{r build, eval = FALSE}
devtools::build()
```

Sometimes, on some ONS systems, this doesn't work in which case you can use the "Install and Restart" button under the Build tab of the GUI. If this doesn't work, try setting the system environment variable for RTools using

```{r rtoolsenv, eval = FALSE}
Sys.setenv(PATH = paste("C:/Rtools/bin", Sys.getenv("PATH"), sep = ";"))
Sys.setenv(BINPREF = "C:/Rtools/mingw_$(WIN)/bin/")
```

Where `C:/Rtools` is the appropriate path to your RTools installation.

# Development Learning

Developing this package further will require a working knowledge of several packages. Detailed below are several links to books, packages and vignettes.

* Tools to make an R developer's life easier: [devtools](https://github.com/r-lib/devtools)
* Keep your code and code documentation together using [Roxygen2](https://github.com/klutometis/roxygen): Check out the vignettes [here](https://cran.r-project.org/web/packages/roxygen2/)
* Test your code with [testthat](https://github.com/r-lib/testthat)

You can read about these tools and the wider R package development world in the [R Packages](http://r-pkgs.had.co.nz) book.

Key functions to know about are:

* devtools::document()
* devtools::test()
* devtools::check()
* devtools::build()

A very good (free) book on general R can be found [here](https://adv-r.hadley.nz).

All of the packages I have mentioned come from the "[tidyverse](https://www.tidyverse.org)" which is a collection of packages that work very well together. A key package which works well with [sparklyr](http://spark.rstudio.com) is called [dplyr](http://dplyr.tidyverse.org). dplyr provides a grammar of data manipulation, providing a consistent set of verbs that solve the most common data manipulation challenges. It is the package to use for data manipulation in R and is R's version of the Python pandas library.

# Generating HTML Documentation

We can generate the HTML from an Rd file using the following code.

```{r build_docs}
tools::Rd2HTML("../man/scala_list.Rd")
```

This is linked to JIRA ticket [DAPS-433](https://collaborate2.ons.gov.uk/jira/browse/DAPS-433). However a more efficient way to do this is by using the `knitr` package.

```{r build_docs_knitr, eval = FALSE}
knitr::knit_rd("sparkts")
```

This will generate html files for all of the documentation in your package as well as an index.html and the css.

# Connecting to Spark

In order to [connect to Spark on the Cloudera Data Science Workbench (CDSW)](https://blog.cloudera.com/blog/2017/09/how-to-distribute-your-r-code-with-sparklyr-and-cdsw/), the user must configure their connection using the `sparklyr` package. An example of this can be seen below.

```{r spark_cloudera, eval = FALSE}
library(sparklyr)

config <- spark_config()
config[["spark.r.command"]] <- "/usr/local/lib/R/bin/Rscript"
config$sparklyr.apply.env.R_HOME <- "/usr/local/lib/R"
config$sparklyr.apply.env.R_SHARE_DIR <- "/usr/local/lib/R/share"
config$sparklyr.apply.env.R_INCLUDE_DIR <- "/usr/local/lib/R/include"

sc <- spark_connect(master = "yarn-client", config = config)
```

This configuration has been handily wrapped into a function called `cloudera_spark_connect()`. 

```{r connect_cloudera, eval = FALSE}
sc <- cloudera_spark_connect()
```

This is linked to JIRA ticket [DAPS-450](https://collaborate2.ons.gov.uk/jira/browse/DAPS-450).

Should you need any additional packages installing on CDSW you can use the following code:

```{r install_pkgs_cloudera, eval = FALSE}
install.packages("sparklyr", repos = "http://username:pass@art-p-01/artifactory/vr-R")
```

Where `username` and `pass` are your username and passwords for Artifactory.

# API

The functions in this package generally begin with `sdf_*`. This stands for Spark DataFrame and is used for two reasons:

1. It extends the same API used by the `sparklyr` package
2. We are using Spark DataFrames and modifying them in place


A decision was made to use generic funtions (and possibly S3 classes where needed) over R6 classes since generics are more typical of the R code the end user will be used to. This is detailed more [here](http://np2rvlapxx507/DAP-S/sparkts-R/blob/master/inst/docs/api_decision.md).

# Testing

```{r spark, cache = TRUE, include = FALSE}
sc <- sparklyr::spark_connect(
  master = "local", 
  version = "2.2.0", 
  config = list(sparklyr.gateway.address = "127.0.0.1")
)

# Define the expected data
expected_df <- structure(
  list(
    ref = c(
      "000000000", "111111111", "222222222", 
      "333333333", "444444444", "555555555", "666666666", "777777777"
    ), 
    xColumn = c(
      "200", "300", "400", "500", "600", "700", "800", "900"), 
    yColumn = c(120, 220, 320, 420, 520, 620, 720, 820), 
    zColumn = c(10, 20, 30, 40, 53, 60, 70, 80), 
    stdError = c(
      10.5851224804993, 14.1156934648117, 16.7967753286756, 19.0703353574777, 
      22.351729734926, 22.9194448813965, 24.6125909283282, 26.1943338370632)), 
  .Names = c(
    "ref", "xColumn", "yColumn", "zColumn", "stdError"), 
  row.names = c(NA, -8L), 
  class = c("tbl_df", "tbl", "data.frame")
)

# Read in the data
std_data <- sparklyr::spark_read_json(
  sc,
  "std_data",
  path = system.file(
    "data_raw/StandardErrorDataIn.json",
    package = "sparkts"
  )
) %>%
  sparklyr::spark_dataframe()

# Call the method
output <- sdf_standard_error(
  sc = sc, data = std_data,
  x_col = "xColumn", y_col = "yColumn", z_col = "zColumn",
  new_column_name = "stdError"
) %>%
  dplyr::collect()
```

You may see the following issue when testing:

```{r fail, error = TRUE}
expect_identical(
  output,
  expected_df
)
```

If you use `dput` to generate your expected output, it doesn't store the full numeric data information. To prove this, rounding this information allows the test to pass:

```{r round}
expect_identical(
  output %>% dplyr::mutate(stdError = round(stdError, 2)),
  expected_df %>% dplyr::mutate(stdError = round(stdError, 2))
)
```


If you must use `dput`, one way we can keep the full numeric data information is using hexadecimal (binary fractions) data. See `?deparseOpts` for more information.

```{r expectedCorrect}
expected_df <- dput(
  output, control = c("keepNA", "keepInteger", "showAttributes", "hexNumeric")
)
```

Then when we compare the two datasets, we see no errors.

```{r pass}
expect_identical(
  output,
  expected_df
)
```


If you donâ€™t want to use hexadecimal units, for whatever reason, you can get away with using `expect_equal()` instead of `expect_identical()` which adds a tolerance to numerical value comparisons. See `?all.equal` for more information.

You can always just import the data from a JSON file using `sparklyr`, however.

```{r expectJSON, eval = FALSE}
expected_df_json <- sparklyr::spark_read_json(
  sc,
  "std_data",
  path = system.file(
    "path/to/jsonfile.json",
    package = "sparkts"
  )
) %>%
  sparklyr::spark_dataframe()
```
